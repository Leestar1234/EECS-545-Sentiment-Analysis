{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a0e93bf",
   "metadata": {
    "cell_id": 40
   },
   "source": [
    "### 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bd6e538",
   "metadata": {
    "cell_id": 3,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_all = pd.read_csv('data.three_class.csv')\n",
    "df_test = df_all[2716:3397]\n",
    "df_train = pd.concat([df_all[0:2716], df_all[3397:3397]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54d8d9af-a336-4dc7-903e-d1eb113d72d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import packages. Run this cell.\n",
    "import numpy as np\n",
    "import numpy.random as rn\n",
    "from numpy.core.fromnumeric import shape\n",
    "from pyvis import network as net\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b61adf5",
   "metadata": {
    "cell_id": 42
   },
   "source": [
    "### 训练词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b28fcb74",
   "metadata": {
    "cell_id": 44,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [, , , 冬奥, 事, 冰壶, 比赛, , 充满, 尖叫, , 摩擦, O, , , ,...\n",
       "1    [包头, 疫情, 北京, 冬奥, 令人, 一生, 难忘, , , , , 刷到, 家门口, ...\n",
       "2             [肖战, 期待, 冬奥, 赛场, , , , 抹, 中国, 红, 加油, 加油]\n",
       "3                               [北京, 冬奥会, 闭幕式, 冬奥, 再见]\n",
       "4                           [北京, 冬奥会, 闭幕式, 期待, 下次, 冬奥]\n",
       "Name: context, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word2vec要求的输入格式: list(word)\n",
    "wv_input = df_all['context'].map(lambda s: s.split(\" \"))   # [for w in s.split(\" \") if w not in stopwords]\n",
    "wv_input.head()                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ca73bd8",
   "metadata": {
    "cell_id": 4,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "\n",
    "# Word2Vec\n",
    "word2vec = models.Word2Vec(wv_input, \n",
    "                           vector_size=32,   \n",
    "                           min_count=1,      \n",
    "                           epochs=1000)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab524449",
   "metadata": {
    "cell_id": 48
   },
   "source": [
    "### 神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "b4c18bd5",
   "metadata": {
    "cell_id": 14,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence,pad_packed_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "#device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "e9f8576f",
   "metadata": {
    "cell_id": 19,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 超参数\n",
    "learning_rate = 5e-4\n",
    "input_size = 300\n",
    "num_epoches = 5\n",
    "batch_size = 1\n",
    "\n",
    "embed_size = 32\n",
    "hidden_size = 32\n",
    "num_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "8b411e2b",
   "metadata": {
    "cell_id": 7,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.74896323e-01 6.24827205e-01 2.76472215e-04]\n"
     ]
    }
   ],
   "source": [
    "# 数据集\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.data = []\n",
    "        self.label = []\n",
    "        self.count = np.ones([len(word2vec.wv.key_to_index),3])\n",
    "        self.word_in_w2v = []\n",
    "        for t in df[\"emotion\"].tolist():\n",
    "            if t == 1:\n",
    "                vec = [1, 0, 0]\n",
    "            elif t == 0:\n",
    "                vec = [0, 1, 0]\n",
    "            else:\n",
    "                vec = [0, 0, 1]\n",
    "            '''\n",
    "            if t == -1:\n",
    "                vec = 0\n",
    "            else:\n",
    "                vec = 1\n",
    "            '''\n",
    "            self.label.append(vec)\n",
    "        i = 0\n",
    "        for s in df[\"context\"].tolist():\n",
    "            vectors = []\n",
    "            sentence = []\n",
    "            for w in s.split(\" \"):\n",
    "                if w in word2vec.wv.key_to_index:\n",
    "                    x = word2vec.wv[w]\n",
    "                    vectors.append(x)  \n",
    "                    sentence.append(w)\n",
    "                    self.count[word2vec.wv.key_to_index[w],self.label[i]] += 1\n",
    "            vectors = torch.Tensor(vectors)\n",
    "            self.data.append(vectors)\n",
    "            self.word_in_w2v.append(sentence)\n",
    "            i += 1\n",
    "        #print(word2vec.wv.key_to_index.keys())\n",
    "        #print(np.sum(self.count,axis = 1)[0:300])\n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        label = self.label[index]\n",
    "        word = self.word_in_w2v[index]\n",
    "        return data, label ,word\n",
    "    def getprob(self, words):\n",
    "        D = np.zeros([3])\n",
    "        if words not in word2vec.wv.key_to_index.keys():\n",
    "            return 0,0\n",
    "        D += self.count[word2vec.wv.key_to_index[words]]\n",
    "        q = np.sum(D)\n",
    "        D = D / [0.5,0.3,0.2]\n",
    "        p = D / np.sum(D)\n",
    "        return p,q\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "        \n",
    "\n",
    "\n",
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "    :param data: 第0维：data，第1维：label\n",
    "    :return: 序列化的data、记录实际长度的序列、以及label列表\n",
    "    \"\"\"\n",
    "    data.sort(key=lambda x: len(x[0]), reverse=True) \n",
    "    data_length = [len(sq[0]) for sq in data]\n",
    "    x = [i[0] for i in data]\n",
    "    y = [i[1] for i in data]\n",
    "    words = [i[2] for i in data]\n",
    "    data = pad_sequence(x, batch_first=True, padding_value=0)   \n",
    "    return data, torch.tensor(y, dtype=torch.float32), data_length, words\n",
    "\n",
    "def reward_fun(out_0,action,out_1,dic_dis, freq, alpha = 10):\n",
    "    if freq < 50:\n",
    "        reward = 0\n",
    "        return reward\n",
    "    else:\n",
    "        dic_reward = -alpha * (np.sum(action * np.log(dic_dis)))\n",
    "        out_0 = out_0.squeeze()\n",
    "        out_1 = out_1.squeeze()\n",
    "        policy_reward = (out_1[np.argmax(action)] - out_0[np.argmax(action)])\n",
    "        reward = dic_reward + policy_reward\n",
    "        return reward.detach().numpy()\n",
    "def dictionary(word):\n",
    "    s,t = train_data.getprob(word)\n",
    "    if t >= 40:\n",
    "        return s,t\n",
    "    else:\n",
    "        s = [0,1,0]\n",
    "        return s,t\n",
    "# 训练集\n",
    "train_data = MyDataset(df_train)\n",
    "p = train_data.__getitem__(5)[0]\n",
    "q = train_data.__getitem__(5)[2]\n",
    "train_data.getprob(' ')\n",
    "s,t = dictionary('冬奥')\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "\n",
    "test_data = MyDataset(df_test)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "9225c131-d605-4849-8a23-50453e91ede2",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = TD3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "31b73f08",
   "metadata": {
    "cell_id": 11,
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 网络结构\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=False)\n",
    "        self.lstm2 = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=False)\n",
    "        self.lstm3 = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=False)\n",
    "        self.fc = nn.Linear(hidden_size, 3)  \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, lengths, words ,train = True):\n",
    "        global cur_time_step\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)  \n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        packed_input = torch.nn.utils.rnn.pack_padded_sequence(input=x, lengths=lengths, batch_first=True)\n",
    "        # find closest word\n",
    "        \n",
    "        x_temp = torch.zeros([1,1,32])\n",
    "        h_n, h_c = h0,c0\n",
    "        action_list = []\n",
    "        for k in range(lengths[0]):\n",
    "            x_temp[0,0] = packed_input[0][k]\n",
    "            packed_input_temp = torch.nn.utils.rnn.pack_padded_sequence(input=x_temp, lengths=[1], batch_first=True)\n",
    "            out_0 = self.fc(h_n)\n",
    "            out_0 = self.sigmoid(out_0)\n",
    "            #get action distribution from actor net\n",
    "            state = torch.cat([h_n.flatten(), x_temp.flatten()],dim = 0).detach().numpy()\n",
    "            dic_dis, freq = dictionary(words[0][k])\n",
    "            if words[0][k] == '':\n",
    "                action = [0,1,0]\n",
    "            elif freq >= 50:\n",
    "                action = agent.policy(state)\n",
    "            else:\n",
    "                action = [0,1,0]\n",
    "            done = 0\n",
    "            i = np.random.choice([0,1,2],1,p=action)\n",
    "            #print(action)\n",
    "            action_list.append(np.sum(action*np.array([1,0,-1])))                  \n",
    "                                    \n",
    "            #train the policy network\n",
    "            \n",
    "            if i == 0:\n",
    "                packed_out, (h_n, h_c) = self.lstm1(packed_input_temp, (h_n, h_c))\n",
    "            elif i == 1:\n",
    "                packed_out, (h_n, h_c) = self.lstm2(packed_input_temp, (h_n, h_c))\n",
    "            elif i == 2:\n",
    "                packed_out, (h_n, h_c) = self.lstm3(packed_input_temp, (h_n, h_c))\n",
    "            \n",
    "            #find the next state\n",
    "            \n",
    "            out_1 = self.fc(h_n)\n",
    "            out_1 = self.sigmoid(out_1)\n",
    "            \n",
    "            if k ==lengths[0]-1:\n",
    "                done = 1\n",
    "                next_state = torch.cat([h_n.flatten(), x_temp.flatten()],dim = 0).detach().numpy()\n",
    "            else:\n",
    "                x_temp[0,0] = packed_input[0][k+1]\n",
    "                next_state = torch.cat([h_n.flatten(), x_temp.flatten()],dim = 0).detach().numpy()\n",
    "            reward = reward_fun(out_0,action,out_1,dic_dis, freq)\n",
    "            if train == True:\n",
    "                if freq >= 50:\n",
    "                    if words[0][k] != '':\n",
    "                        agent.train(cur_time_step, state, action, reward, next_state, done)\n",
    "            \n",
    "            \n",
    "            cur_time_step += 1\n",
    "        lstm_out = h_n  \n",
    "        #print(lstm_out.shape,x.shape)\n",
    "        out = self.fc(lstm_out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out,action_list\n",
    "\n",
    "lstm = LSTM(embed_size, hidden_size, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "0a5f4edd-9395-4118-aa34-9e5526f00b2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Some parameters\n",
    "\"\"\"\n",
    "state_size = 64  # state dimension\n",
    "action_size = 3  # action dimension\n",
    "fc_units = 32  # number of neurons in one fully connected hidden layer\n",
    "action_upper_bound = 1  # action space upper bound\n",
    "action_lower_bound = -1  # action space lower bound\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Structure of Actor Network.\n",
    "\"\"\"\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.max_action = action_upper_bound\n",
    "        self.fc1 = nn.Linear(state_size, fc_units)\n",
    "        self.fc2 = nn.Linear(fc_units, 3)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Build an actor (policy) network that maps states -> actions.\n",
    "        Args:\n",
    "            state: torch.Tensor with shape (batch_size, state_size)\n",
    "        Returns:\n",
    "            action: torch.Tensor with shape (batch_size, action_size)\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = self.fc2(x)\n",
    "        action = self.sigmoid(x)\n",
    "        action = action/torch.sum(action)\n",
    "        return action\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Structure of Critic Network.\n",
    "\"\"\"\n",
    "class CriticQ(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state_size: state dimension\n",
    "            action_size: action dimension\n",
    "            fc_units: number of neurons in one fully connected hidden layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Q-network 1 architecture\n",
    "        self.l1 = nn.Linear(state_size + action_size, fc_units)\n",
    "        self.l2 = nn.Linear(fc_units, fc_units)\n",
    "        self.l3 = nn.Linear(fc_units, 1)\n",
    "\n",
    "        # Q-network 2 architecture\n",
    "        self.l4 = nn.Linear(state_size + action_size, fc_units)\n",
    "        self.l5 = nn.Linear(fc_units, fc_units)\n",
    "        self.l6 = nn.Linear(fc_units, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"\n",
    "        Build a critic (value) network that maps state-action pairs -> Q-values.\n",
    "        Args:\n",
    "            state: torch.Tensor with shape (batch_size, state_size)\n",
    "            action: torch.Tensor with shape (batch_size, action_size)\n",
    "        Returns:\n",
    "            Q_value_1: torch.Tensor with shape (batch_size, 1)\n",
    "            Q_value_2: torch.Tensor with shape (batch_size, 1)\n",
    "        \"\"\"\n",
    "        state_action = torch.cat([state, action], 1)\n",
    "        \n",
    "        x1 = F.relu(self.l1(state_action))\n",
    "        x1 = F.relu(self.l2(x1))\n",
    "        Q_value_1 = self.l3(x1)\n",
    "        \n",
    "        x2 = F.relu(self.l4(state_action))\n",
    "        x2 = F.relu(self.l5(x2))\n",
    "        Q_value_2 = self.l6(x2)\n",
    "        \n",
    "        return Q_value_1, Q_value_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "672313b0-0a26-4995-b5f9-4b6bc29f6be1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of TD3 Algorithm\n",
    "\"\"\"\n",
    "import random\n",
    "class TD3:\n",
    "    def __init__(self):\n",
    "        self.lr_actor = 1e-2  # learning rate for actor network\n",
    "        self.lr_critic = 1e-2  # learning rate for critic network\n",
    "        self.buffer_capacity = 100000  # replay buffer capacity\n",
    "        self.batch_size = 128  # mini-batch size\n",
    "        self.tau = 0.02  # soft update parameter\n",
    "        self.policy_delay = 2  # policy will be updated once every policy_delay times for each update of the Q-networks.\n",
    "        self.gamma = 0.99  # discount factor\n",
    "        self.target_noise = 0.2  # standard deviation for smoothing noise added to target policy\n",
    "        self.noise_clip = 0.5  # limit for absolute value of target policy smoothing noise.\n",
    "        self.update_every = 200  # number of env interactions that should elapse between updates of Q-networks.\n",
    "        # Note: Regardless of how long you wait between updates, the ratio of env steps to gradient steps should be 1.\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.device = torch.device(\"cpu\")  # or self.device = torch.device(\"cuda\")\n",
    "        self.action_upper_bound = action_upper_bound  # action space upper bound\n",
    "        self.action_lower_bound = action_lower_bound  # action space lower bound\n",
    "        self.create_actor()\n",
    "        self.create_critic()\n",
    "        self.act_opt = optim.Adam(self.actor.parameters(), lr=self.lr_actor)\n",
    "        self.crt_opt = optim.Adam(self.critic.parameters(), lr=self.lr_critic)\n",
    "        self.replay_memory_buffer = deque(maxlen=self.buffer_capacity)\n",
    "        \n",
    "    def create_actor(self):\n",
    "        self.actor = Actor().to(self.device)\n",
    "        self.actor_target = Actor().to(self.device)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "\n",
    "    def create_critic(self):\n",
    "        self.critic = CriticQ().to(self.device)\n",
    "        self.critic_target = CriticQ().to(self.device)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "    \n",
    "    def add_to_replay_memory(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Add samples to replay memory\n",
    "        Args:\n",
    "            state: current state, a numpy array with shape (state_size,)\n",
    "            action: current action, a numpy array with shape (action_size,)\n",
    "            reward: reward obtained\n",
    "            next_state: next state, a numpy array with shape (state_size,)\n",
    "            done: True when the current episode ends, False otherwise\n",
    "        \"\"\"\n",
    "        self.replay_memory_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def get_random_sample_from_replay_mem(self):\n",
    "        \"\"\"\n",
    "        Random samples from replay memory without replacement\n",
    "        Returns a self.batch_size length list of unique elements chosen from the replay buffer.\n",
    "        Returns:\n",
    "            random_sample: a list with len=self.batch_size,\n",
    "                           where each element is a tuple (state, action, reward, next_state, done)\n",
    "        \"\"\"\n",
    "        random_sample = random.sample(self.replay_memory_buffer, self.batch_size)\n",
    "        return random_sample\n",
    "    \n",
    "    def soft_update_target(self, local_model, target_model):\n",
    "        \"\"\"\n",
    "        Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Args:\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(self.tau*local_param.data + (1.0-self.tau)*target_param.data)\n",
    "\n",
    "    def train(self, cur_time_step, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Collect samples and update actor network and critic network using mini-batches of experience tuples.\n",
    "        Args:\n",
    "            cur_time_step: current time step counting from the beginning, \n",
    "                           which is equal to the number of times the agent interacts with the environment\n",
    "            episode_time_step: the time step counting from the current episode\n",
    "            state: current state, a numpy array with shape (state_size,)\n",
    "            action: current action, a numpy array with shape (action_size,)\n",
    "            reward: reward obtained\n",
    "            next_state: next state, a numpy array with shape (state_size,)\n",
    "            done: True when the current episode ends, False otherwise\n",
    "        \"\"\"\n",
    "        self.add_to_replay_memory(state, action, reward, next_state, done)      \n",
    "        if len(self.replay_memory_buffer) < self.batch_size:\n",
    "            return\n",
    "        if cur_time_step % self.update_every != 0:\n",
    "            return\n",
    "        \n",
    "        # Perform self.update_every times of updates of the critic networks and \n",
    "        # (self.update_every / policy_delay) times of updates of the actor network\n",
    "        for it in range(self.update_every): \n",
    "            \"\"\"\n",
    "            state_batch: torch.Tensor with shape (self.batch_size, state_size), a mini-batch of current states\n",
    "            action_batch: torch.Tensor with shape (self.batch_size, action_size), a mini-batch of current actions\n",
    "            reward_batch: torch.Tensor with shape (self.batch_size, 1), a mini-batch of rewards\n",
    "            next_state_batch: torch.Tensor with shape (self.batch_size, state_size), a mini-batch of next states\n",
    "            done_list: torch.Tensor with shape (self.batch_size, 1), a mini-batch of 0-1 integers, \n",
    "                   where 1 means the episode terminates for that sample;\n",
    "                         0 means the episode does not terminate for that sample.\n",
    "            \"\"\"\n",
    "            mini_batch = self.get_random_sample_from_replay_mem()\n",
    "            state_batch = torch.from_numpy(np.vstack([i[0] for i in mini_batch])).float().to(self.device)\n",
    "            action_batch = torch.from_numpy(np.vstack([i[1] for i in mini_batch])).float().to(self.device)\n",
    "            reward_batch = torch.from_numpy(np.vstack([i[2] for i in mini_batch])).float().to(self.device)\n",
    "            next_state_batch = torch.from_numpy(np.vstack([i[3] for i in mini_batch])).float().to(self.device)\n",
    "            done_list = torch.from_numpy(np.vstack([i[4] for i in mini_batch]).astype(np.uint8)).float().to(self.device)\n",
    "            \n",
    "\n",
    "            current_q = self.critic(state_batch,action_batch)\n",
    "            next_action_batch = self.actor_target(next_state_batch) + torch.clamp(torch.normal(0,0.2,action_batch.shape),-self.noise_clip,self.noise_clip)\n",
    "            target_q1 = reward_batch + torch.mul(torch.mul(self.critic_target(next_state_batch,next_action_batch)[0].detach(),self.gamma),1-done_list)\n",
    "            target_q2 = reward_batch + torch.mul(torch.mul(self.critic_target(next_state_batch,next_action_batch)[1].detach(),self.gamma),1-done_list)\n",
    "            \n",
    "            y = torch.min(target_q1,target_q2)\n",
    "            loss = ((torch.sum((y - current_q[0])**2) + torch.sum((y - current_q[1])**2))).mean()\n",
    "            self.crt_opt.zero_grad()\n",
    "            loss.backward()\n",
    "            self.crt_opt.step()\n",
    "\n",
    "           \n",
    "\n",
    "            if it % self.policy_delay == 0:\n",
    "                \n",
    "\n",
    "                \"\"\"\n",
    "                Hint: \n",
    "                  You may update self.actor using the optimizer self.act_opt and recall the loss function for DDPG training\n",
    "                \"\"\"\n",
    "\n",
    "                actor = self.actor(state_batch)\n",
    "                q1,q2 = self.critic(state_batch,actor)\n",
    "                loss = -sum(q1+q2)\n",
    "                self.act_opt.zero_grad()\n",
    "                loss.backward()\n",
    "                self.act_opt.step()\n",
    "\n",
    "                \n",
    "                # Soft update target models\n",
    "                self.soft_update_target(self.critic, self.critic_target)\n",
    "                self.soft_update_target(self.actor, self.actor_target)\n",
    "            \n",
    "    \n",
    "    def policy(self, state):\n",
    "        \"\"\"\n",
    "        Select action based on the actor network.\n",
    "        Args:\n",
    "            state: a numpy array with shape (state_size,)\n",
    "        Returns:\n",
    "            actions: a numpy array with shape (action_size,)\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "        self.actor.eval()\n",
    "        with torch.no_grad():\n",
    "            actions = np.squeeze(self.actor(state).cpu().data.numpy())\n",
    "        self.actor.train()\n",
    "        return actions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "73507bf1",
   "metadata": {
    "cell_id": 26,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def test():\n",
    "    y_pred, y_true = [], []\n",
    "\n",
    "    for x, labels, lengths, words in test_loader:\n",
    "        x = x.to(device)\n",
    "        outputs,_ = lstm(x, lengths,words,train = False)\n",
    "        outputs = outputs.detach()\n",
    "        outputs = outputs.view(-1,3)          \n",
    "        y_pred.append(outputs)\n",
    "        y_true.append(labels)\n",
    "\n",
    "    y_prob = torch.cat(y_pred)\n",
    "    y_label = torch.cat(y_true)\n",
    "    y_pred = []\n",
    "    for s in y_prob:\n",
    "        if torch.argmax(s) == 0:\n",
    "            lab = 0\n",
    "        elif torch.argmax(s) == 1:\n",
    "            lab = 1\n",
    "        else:\n",
    "            lab = 2\n",
    "        y_pred.append(lab)\n",
    "    y_true = []\n",
    "    for i in y_label:\n",
    "        if torch.argmax(i) == 0:\n",
    "            lab = 0\n",
    "        elif torch.argmax(i) == 1:\n",
    "            lab = 1\n",
    "        else:\n",
    "            lab = 2\n",
    "        y_true.append(lab)\n",
    "    \n",
    "    print(\"Accuracy score:\", metrics.accuracy_score(y_true, y_pred))\n",
    "    print(\"f-1 score:\", metrics.f1_score(y_true, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "d6ded49d",
   "metadata": {
    "cell_id": 32,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义损失函数和优化器\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "2a281e54",
   "metadata": {
    "cell_id": 33,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.5617647058823529\n",
      "f-1 score: 0.5085022779290606\n",
      "Accuracy score: 0.6220588235294118\n",
      "f-1 score: 0.5763047732978938\n",
      "Accuracy score: 0.6617647058823529\n",
      "f-1 score: 0.6405118958909942\n",
      "Accuracy score: 0.6735294117647059\n",
      "f-1 score: 0.6485971032729823\n",
      "Accuracy score: 0.6838235294117647\n",
      "f-1 score: 0.6582866530058815\n"
     ]
    }
   ],
   "source": [
    "# 迭代训练\n",
    "cur_time_step = 0\n",
    "for epoch in range(5):\n",
    "    total_loss = 0\n",
    "    for i, (x, labels, lengths,words) in enumerate(train_loader):\n",
    "        x = x.to(device)\n",
    "        #print(x.shape,lengths)\n",
    "        #print(len(words[0]))\n",
    "        labels = labels.to(device)\n",
    "        outputs,_ = lstm(x, lengths,words,train = False)          \n",
    "        logits = outputs.view(-1,3)           \n",
    "        loss = criterion(logits, labels)    \n",
    "        total_loss += loss\n",
    "        optimizer.zero_grad()               \n",
    "        loss.backward(retain_graph=True)    \n",
    "        optimizer.step()                    \n",
    "        if (i+1) % 10 == 0:\n",
    "            total_loss = 0\n",
    "    # test\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "c672b5ad-abef-4f99-adf9-41e0fa376413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sentence():\n",
    "    q = test_data.__getitem__(20)\n",
    "    a = q[0].view([1,q[0].shape[0],q[0].shape[1]])\n",
    "    b = len(a[0])\n",
    "    c = np.zeros(q[0].shape[0])\n",
    "    print(q[2])\n",
    "    print(q[0].shape[0])\n",
    "    for i in range(1):\n",
    "        outputs,sentiment_fluctuation = lstm(a, [b],[q[2]],train = False)\n",
    "        outputs = outputs.detach()\n",
    "        outputs = outputs.view(-1,3)\n",
    "        print(outputs)\n",
    "        c += sentiment_fluctuation\n",
    "    c = c\n",
    "    d = np.concatenate([q[2],c],axis=0)\n",
    "    plt.plot(c)\n",
    "    g = np.argsort(c)\n",
    "    print(c)\n",
    "    print(q[2][g[0]],q[2][g[1]],q[2][g[2]],q[2][g[3]])\n",
    "    print(q[2][g[-5]],q[2][g[-4]],q[2][g[-3]],q[2][g[-2]],q[2][g[-1]])\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "2fb5dfc0-279f-48a6-99b8-110914d48597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['终于', '明白', '中国', '', '', '疫情', '', '严重', '', '', '', '举办', '冬奥会', '', '', '', '', '', '国家', '', '强大', '繁荣昌盛', '', '经济', '', '文化', '', '', '树立', '', '自信', '', '', '激动', '振奋', '', '祖国', '', '未来', '充满', '', '信心', '加油', '', '', '祖国']\n",
      "46\n",
      "tensor([[0.8980, 0.0383, 0.0369]])\n",
      "[-0.0268949   0.         -0.03978494  0.          0.         -0.33033934\n",
      "  0.          0.          0.          0.          0.         -0.22149223\n",
      " -0.08891734  0.          0.          0.          0.          0.\n",
      "  0.04508427  0.          0.          0.          0.          0.\n",
      "  0.          0.04463884  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.01330203  0.          0.\n",
      " -0.08302602  0.         -0.03783742  0.          0.          0.\n",
      "  0.09897283  0.          0.         -0.08899546]\n",
      "疫情 举办 祖国 冬奥会\n",
      " 激动 文化 国家 加油\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_sentence()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "max_cell_id": 55
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
