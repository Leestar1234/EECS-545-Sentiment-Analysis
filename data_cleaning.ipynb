{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train= pd.read_excel('data/data_3500.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=[]\n",
    "\n",
    "with open('data/stopwords.txt','r',encoding = 'utf8') as f:\n",
    "    for w in f:\n",
    "        stopwords.append(w.strip())\n",
    "        \n",
    "def load_corpus(path):\n",
    "    \"\"\"\n",
    "    Load the corpus\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(path, \"r\", encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            [_, seniment, content] = line.split(\",\", 2)\n",
    "            content = processing(content)\n",
    "            data.append((content, int(seniment)))\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_corpus_bert(path):\n",
    "    \"\"\"\n",
    "    Load the corpus\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(path, \"r\", encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            [_, seniment, content] = line.split(\",\", 2)\n",
    "            content = processing_bert(content)\n",
    "            data.append((content, int(seniment)))\n",
    "    return data\n",
    "\n",
    "def get_stopword_list(file):\n",
    "    with open(file,'r',encoding = 'utf-8') as f:\n",
    "        stopword_list = [word.strip('\\n') for word in f.readlines()]\n",
    "        return stopword_list\n",
    "    \n",
    "def clean_stopword(str, stopword_list):\n",
    "    result = ''\n",
    "    word_list = jieba.lcut(str)\n",
    "    for w in word_list:\n",
    "        if w not in stopword_list:\n",
    "            result += w\n",
    "    return result\n",
    "    \n",
    "def processing(text):\n",
    "    \"\"\"\n",
    "    data preprocessing\n",
    "    \"\"\"\n",
    "    # datacleaning\n",
    "    text = re.sub(\"\\{%.+?%\\}\", \" \", text)           # remove {%xxx%} (Geolocation, Microblogging topics, etc)\n",
    "    text = re.sub(\"@.+?( |$)\", \" \", text)           # remove @xxx (user name)\n",
    "    text = re.sub(\"【.+?】\", \" \", text)              # remove 【xx】 (content not written by user)\n",
    "    text = re.sub(\"\\u200b\", \" \", text)              # '\\u200b'\n",
    "    # 分词\n",
    "    words = [w for w in jieba.lcut(text) if w.isalpha()]\n",
    "    #  splice [\"不\" = No]  with the word after it\n",
    "    while \"不\" in words:\n",
    "        index = words.index(\"不\")\n",
    "        if index == len(words) - 1:\n",
    "            break\n",
    "        words[index: index+2] = [\"\".join(words[index: index+2])]  # list splice\n",
    "    # Concatenate strings with Spaces\n",
    "    result = \" \".join(words)\n",
    "    return result\n",
    "\n",
    "\n",
    "def processing_bert(text):\n",
    "\n",
    "    # data cleaning\n",
    "    text = re.sub(\"\\{%.+?%\\}\", \" \", text)           # remove {%xxx%} (Geolocation, Microblogging topics, etc)\n",
    "    text = re.sub(\"@.+?( |$)\", \" \", text)          # remove @xxx (user name)\n",
    "    text = re.sub(\"【.+?】\", \" \", text)            # remove 【xx】 (content not written by user)         \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(df_train)):\n",
    "    df_train['context'][i] = processing(df_train['context'][i])\n",
    "    df_train['context'][i] = clean_stopword(df_train['context'][i],stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# three classes cleaned data\n",
    "df_train.to_csv('three_class.csv',encoding='utf_8_sig')\n",
    "\n",
    "\n",
    "# two classes cleaned data\n",
    "df_train = df_train.drop(df_train[df_train['emotion'] == 0].index)\n",
    "df_train = df_train.dropna(axis=0, how = 'all')\n",
    "df_train.to_csv('two_class.csv',encoding = \"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
