{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac347deb-1e78-41f7-94a2-fd9e1a891354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from tqdm.auto import tqdm, trange\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a6a3bffc-03f9-4c9b-9802-56b706a962d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original length 3409\n",
      "After dropping 3396\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>冬奥 事 冰壶 比赛  充满 尖叫  摩擦 O    冬奥 事 冰壶 比赛  充满 尖...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>包头 疫情 北京 冬奥 令人 一生 难忘     刷到 家门口  疫情</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>肖战 期待 冬奥 赛场    抹 中国 红 加油 加油</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>北京 冬奥会 闭幕式 冬奥 再见</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>北京 冬奥会 闭幕式 期待 下次 冬奥</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  emotion\n",
       "0     冬奥 事 冰壶 比赛  充满 尖叫  摩擦 O    冬奥 事 冰壶 比赛  充满 尖...        0\n",
       "1                包头 疫情 北京 冬奥 令人 一生 难忘     刷到 家门口  疫情        0\n",
       "2                        肖战 期待 冬奥 赛场    抹 中国 红 加油 加油        1\n",
       "3                                   北京 冬奥会 闭幕式 冬奥 再见        0\n",
       "4                                北京 冬奥会 闭幕式 期待 下次 冬奥        1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('use_data/data_3500_cleaned.csv')\n",
    "data = data[['context', 'emotion']]\n",
    "print('Original length', len(data))\n",
    "data = data.dropna()\n",
    "print('After dropping', len(data))\n",
    "data['emotion'] = data['emotion'].astype('int64')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4eb6c602-cb08-4318-954c-53d7e34259cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = []\n",
    "with open(\"data/stopwords.txt\", \"r\", encoding=\"utf8\") as f:\n",
    "    for w in f:\n",
    "        stopwords.append(w.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39c500f-cd1b-4345-9265-b7048ce9126b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_2 is used for binary classification\n",
    "data_2 = data[(data['emotion'] == -1) | (data['emotion'] == 1)].reset_index()\n",
    "data_2['emo'] = data_2['emotion'].map({-1:0, 1:1})\n",
    "data_2 = data_2[['context','emo']]\n",
    "print(len(data_2))\n",
    "\n",
    "# data_3 is used for three-class classification\n",
    "data_3 = data\n",
    "data_3['emo'] = data_3['emotion'].map({-1:0, 0:1, 1:2})\n",
    "data_3 = data_3[['context','emo']]\n",
    "print(len(data_3))\n",
    "\n",
    "dataset_2 = np.array(data_2)\n",
    "dataset_3 = np.array(data_3)\n",
    "dataset_3[90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "a068a2e1-f09d-4ce7-9e94-f50b39054953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the pre-trained word embedding from   https://github.com/Embedding/Chinese-Word-Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f405a3f1-109d-429c-bb34-d44f40a5caf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the input of the model\n",
    "class Corpus:\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        \n",
    "        self.comment_list = []\n",
    "\n",
    "        self.word_to_index = {} # word to unique-id\n",
    "        self.index_to_word = {} # unique-id to word\n",
    "\n",
    "        # How many times each word occurs in our data after filtering\n",
    "        self.word_counts = Counter()\n",
    "        self.paired_num = 0  # to store how many words are paired in the pretrained word vectors\n",
    "        \n",
    "        self.word_embedd = np.array  #300 is the embedd dimension which is fixed here\n",
    "        self.result = []\n",
    "        self.data_train = []\n",
    "        self.x_input = []\n",
    "\n",
    "    def load_data(self, data_arr, stop_words):        \n",
    "\n",
    "        # 1: Tokennize and do stop words removal\n",
    "        for n in range(len(data_arr)):\n",
    "            words = data_arr[n].split()\n",
    "            # remove stopwords\n",
    "            words = [w for w in words if w not in stop_words]\n",
    "            self.comment_list.append(words)\n",
    "            \n",
    "        # Construct the whole word list and do stop words removing\n",
    "        total_words = []\n",
    "        for w in self.comment_list:\n",
    "            total_words = total_words + w  # load the whole corpus\n",
    "            \n",
    "        self.word_counts = Counter(total_words)\n",
    "            \n",
    "        # 2: Creat word to id mapping\n",
    "        word_list = list(self.word_counts.keys())\n",
    "        for i in range(len(word_list)):\n",
    "            self.word_to_index[word_list[i]] = i\n",
    "            self.index_to_word[i] = word_list[i]\n",
    "        \n",
    "        self.word_to_index.update({'<UNK>': len(self.word_to_index), '<PAD>': len(self.word_to_index)+1})\n",
    "        self.index_to_word.update({len(self.word_to_index): '<UNK>', len(self.word_to_index)+1 :'<PAD>'})\n",
    "            \n",
    "    def load_pre_trained_embedding(self, file_path, dimmension):\n",
    "        self.word_embedd = np.random.rand(len(self.word_to_index), dimmension)  \n",
    "\n",
    "        f = open(file_path, \"r\", encoding='UTF-8')\n",
    "        for i, vec in enumerate(f.readlines()):\n",
    "            vector = vec.strip().split(\" \")\n",
    "            if vector[0] in self.word_to_index:\n",
    "                index = self.word_to_index[vector[0]]\n",
    "                emb = [float(x) for x in vector[1:301]]  # extract the pretrained embedding\n",
    "                self.paired_num += 1\n",
    "                self.word_embedd[index] = np.asarray(emb, dtype='float32')                \n",
    "        f.close()\n",
    "        np.savez_compressed('pretrained_embedd', embeddings=self.word_embedd)\n",
    "                \n",
    "    \n",
    "    def gen_dataset(self, label_arr, pad_size=24):\n",
    "\n",
    "        for c in range(len(self.comment_list)):\n",
    "            \n",
    "            comment = self.comment_list[c]\n",
    "            comment_id = []\n",
    "            if len(comment) < pad_size:\n",
    "                comment.extend(['<PAD>'] * (pad_size - len(comment)))\n",
    "            else:\n",
    "                comment = comment[:pad_size]\n",
    "            # convert word to id    \n",
    "            for w in comment:\n",
    "                comment_id.append(self.word_to_index[w])\n",
    "            # the format is [([1,2,3],2), ([2,3,4],0),,,,]    \n",
    "            self.result.append((np.array(comment_id), np.array(int(label_arr[c]))))\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "13273199-e7f3-4283-b16e-bf41de1054f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "weibo = Corpus()\n",
    "weibo.load_data(dataset_2[:,0], stopwords)\n",
    "\n",
    "weibo.load_pre_trained_embedding('data/sgns.weibo.char', 300)\n",
    "weibo.gen_dataset(dataset_2[:, 1])\n",
    "\n",
    "len(weibo.word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "90fafd82-59ba-4117-8ac4-8326ff1d0805",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data = weibo.result\n",
    "total_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9e6942eb-09a8-49d9-9bc6-915bc8001d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2324"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fed46fa7-a426-463b-80d0-d8f80e88b020",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dimen, sentence_len, num_filters, dropout):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.conv3 = nn.Conv2d(1, num_filters, (3, embedding_dimen))\n",
    "        self.conv4 = nn.Conv2d(1, num_filters, (4, embedding_dimen))\n",
    "        self.conv5 = nn.Conv2d(1, num_filters, (5, embedding_dimen))\n",
    "        self.Max3_pool = nn.MaxPool2d((sentence_len-3+1, 1))\n",
    "        self.Max4_pool = nn.MaxPool2d((sentence_len-4+1, 1))\n",
    "        self.Max5_pool = nn.MaxPool2d((sentence_len-5+1, 1))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(3*num_filters, 2)   # 2 is the number of class\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch = x.shape[0]\n",
    "        # Convolution\n",
    "        x = x.unsqueeze(1)\n",
    "        x1 = F.relu(self.conv3(x))\n",
    "        x2 = F.relu(self.conv4(x))\n",
    "        x3 = F.relu(self.conv5(x))\n",
    "\n",
    "        # Pooling\n",
    "        x1 = self.Max3_pool(x1)\n",
    "        x2 = self.Max4_pool(x2)\n",
    "        x3 = self.Max5_pool(x3)\n",
    "\n",
    "        # capture and concatenate the features\n",
    "        x = torch.cat((x1, x2, x3), -1)\n",
    "        # print(x.shape)\n",
    "        x = x.view(batch, 1, -1)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # project the features to the labels\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, 2)  # 3 is the number of the label\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2ea8bc7a-2612-493b-b037-3d433d8b8617",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=False)\n",
    "\n",
    "use_data = {}\n",
    "\n",
    "num = 0\n",
    "for train_index, test_index in kf.split(dataset_2):\n",
    "    train = torch.utils.data.DataLoader(dataset=[total_data[i] for i in train_index],    # load the data\n",
    "                                               batch_size=5, \n",
    "                                               shuffle=True)\n",
    "    test = torch.utils.data.DataLoader(dataset=[total_data[i] for i in test_index],    # load the data\n",
    "                                               batch_size=5, \n",
    "                                               shuffle=True)\n",
    "    use_data[num] = [train, test]\n",
    "    num +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3cfe8648-39e3-4c0d-926a-b386df8c7069",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d29e23b9f07f4a9bbeebd13addbd7908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  The loss is: 0.05943\n",
      "epoch 0  The loss is: 0.05160\n",
      "epoch 0  The loss is: 0.05507\n",
      "epoch 1  The loss is: 0.05049\n",
      "epoch 1  The loss is: 0.04886\n",
      "epoch 1  The loss is: 0.04874\n",
      "epoch 1  The loss is: 0.04822\n",
      "epoch 2  The loss is: 0.04588\n",
      "epoch 2  The loss is: 0.04766\n",
      "epoch 2  The loss is: 0.04406\n",
      "epoch 2  The loss is: 0.04338\n",
      "epoch 3  The loss is: 0.04231\n",
      "epoch 3  The loss is: 0.03998\n",
      "epoch 3  The loss is: 0.03761\n",
      "epoch 4  The loss is: 0.04082\n",
      "epoch 4  The loss is: 0.03876\n",
      "epoch 4  The loss is: 0.03765\n",
      "epoch 4  The loss is: 0.03536\n",
      "epoch 5  The loss is: 0.03468\n",
      "epoch 5  The loss is: 0.03733\n",
      "epoch 5  The loss is: 0.03121\n",
      "epoch 5  The loss is: 0.03454\n",
      "epoch 6  The loss is: 0.03572\n",
      "epoch 6  The loss is: 0.03188\n",
      "epoch 6  The loss is: 0.03199\n",
      "epoch 6  The loss is: 0.03287\n",
      "epoch 7  The loss is: 0.03004\n",
      "epoch 7  The loss is: 0.03169\n",
      "epoch 7  The loss is: 0.03001\n",
      "epoch 8  The loss is: 0.03112\n",
      "epoch 8  The loss is: 0.03038\n",
      "epoch 8  The loss is: 0.02895\n",
      "epoch 8  The loss is: 0.03095\n",
      "epoch 9  The loss is: 0.02890\n",
      "epoch 9  The loss is: 0.02556\n",
      "epoch 9  The loss is: 0.02923\n",
      "epoch 9  The loss is: 0.02669\n",
      "epoch 10  The loss is: 0.02744\n",
      "epoch 10  The loss is: 0.02667\n",
      "epoch 10  The loss is: 0.02601\n",
      "epoch 11  The loss is: 0.02403\n",
      "epoch 11  The loss is: 0.02540\n",
      "epoch 11  The loss is: 0.02350\n",
      "epoch 11  The loss is: 0.02333\n",
      "epoch 12  The loss is: 0.02401\n",
      "epoch 12  The loss is: 0.02368\n",
      "epoch 12  The loss is: 0.02442\n",
      "epoch 12  The loss is: 0.02409\n",
      "epoch 13  The loss is: 0.02269\n",
      "epoch 13  The loss is: 0.02086\n",
      "epoch 13  The loss is: 0.02163\n",
      "epoch 13  The loss is: 0.02191\n",
      "epoch 14  The loss is: 0.02278\n",
      "epoch 14  The loss is: 0.02045\n",
      "epoch 14  The loss is: 0.02045\n",
      "epoch 15  The loss is: 0.02177\n",
      "epoch 15  The loss is: 0.02065\n",
      "epoch 15  The loss is: 0.01935\n",
      "epoch 15  The loss is: 0.02013\n",
      "epoch 16  The loss is: 0.01934\n",
      "epoch 16  The loss is: 0.01937\n",
      "epoch 16  The loss is: 0.02070\n",
      "epoch 16  The loss is: 0.01758\n",
      "epoch 17  The loss is: 0.01768\n",
      "epoch 17  The loss is: 0.01880\n",
      "epoch 17  The loss is: 0.01614\n",
      "epoch 18  The loss is: 0.01765\n",
      "epoch 18  The loss is: 0.01776\n",
      "epoch 18  The loss is: 0.01836\n",
      "epoch 18  The loss is: 0.01783\n",
      "epoch 19  The loss is: 0.01556\n",
      "epoch 19  The loss is: 0.01810\n",
      "epoch 19  The loss is: 0.01674\n",
      "epoch 19  The loss is: 0.01495\n",
      "epoch 20  The loss is: 0.01537\n",
      "epoch 20  The loss is: 0.01264\n",
      "epoch 20  The loss is: 0.01488\n",
      "epoch 20  The loss is: 0.01596\n",
      "epoch 21  The loss is: 0.01628\n",
      "epoch 21  The loss is: 0.01394\n",
      "epoch 21  The loss is: 0.01589\n",
      "epoch 22  The loss is: 0.01371\n",
      "epoch 22  The loss is: 0.01552\n",
      "epoch 22  The loss is: 0.01329\n",
      "epoch 22  The loss is: 0.01307\n",
      "epoch 23  The loss is: 0.01267\n",
      "epoch 23  The loss is: 0.01032\n",
      "epoch 23  The loss is: 0.01508\n",
      "epoch 23  The loss is: 0.01230\n",
      "epoch 24  The loss is: 0.01296\n",
      "epoch 24  The loss is: 0.01294\n",
      "epoch 24  The loss is: 0.01233\n",
      "epoch 24  The loss is: 0.01210\n",
      "epoch 25  The loss is: 0.01163\n",
      "epoch 25  The loss is: 0.01072\n",
      "epoch 25  The loss is: 0.01368\n",
      "epoch 26  The loss is: 0.01365\n",
      "epoch 26  The loss is: 0.01098\n",
      "epoch 26  The loss is: 0.01089\n",
      "epoch 26  The loss is: 0.01162\n",
      "epoch 27  The loss is: 0.01111\n",
      "epoch 27  The loss is: 0.01105\n",
      "epoch 27  The loss is: 0.01212\n",
      "epoch 27  The loss is: 0.01052\n",
      "epoch 28  The loss is: 0.01105\n",
      "epoch 28  The loss is: 0.00990\n",
      "epoch 28  The loss is: 0.01170\n",
      "epoch 29  The loss is: 0.01028\n",
      "epoch 29  The loss is: 0.00902\n",
      "epoch 29  The loss is: 0.01082\n",
      "epoch 29  The loss is: 0.01062\n",
      "epoch 30  The loss is: 0.01052\n",
      "epoch 30  The loss is: 0.00999\n",
      "epoch 30  The loss is: 0.01083\n",
      "epoch 30  The loss is: 0.00971\n",
      "epoch 31  The loss is: 0.00931\n",
      "epoch 31  The loss is: 0.00925\n",
      "epoch 31  The loss is: 0.01016\n",
      "epoch 31  The loss is: 0.00889\n",
      "epoch 32  The loss is: 0.01002\n",
      "epoch 32  The loss is: 0.00765\n",
      "epoch 32  The loss is: 0.00855\n",
      "epoch 33  The loss is: 0.00963\n",
      "epoch 33  The loss is: 0.00886\n",
      "epoch 33  The loss is: 0.00868\n",
      "epoch 33  The loss is: 0.00768\n",
      "epoch 34  The loss is: 0.00950\n",
      "epoch 34  The loss is: 0.00883\n",
      "epoch 34  The loss is: 0.00848\n",
      "epoch 34  The loss is: 0.00919\n",
      "epoch 35  The loss is: 0.00827\n",
      "epoch 35  The loss is: 0.00655\n",
      "epoch 35  The loss is: 0.00726\n",
      "epoch 36  The loss is: 0.00871\n",
      "epoch 36  The loss is: 0.00658\n",
      "epoch 36  The loss is: 0.00711\n",
      "epoch 36  The loss is: 0.00688\n",
      "epoch 37  The loss is: 0.00588\n",
      "epoch 37  The loss is: 0.00743\n",
      "epoch 37  The loss is: 0.00641\n",
      "epoch 37  The loss is: 0.00633\n",
      "epoch 38  The loss is: 0.00609\n",
      "epoch 38  The loss is: 0.00710\n",
      "epoch 38  The loss is: 0.00668\n",
      "epoch 38  The loss is: 0.00724\n",
      "epoch 39  The loss is: 0.00657\n",
      "epoch 39  The loss is: 0.00597\n",
      "epoch 39  The loss is: 0.00764\n",
      "epoch 40  The loss is: 0.00764\n",
      "epoch 40  The loss is: 0.00799\n",
      "epoch 40  The loss is: 0.00655\n",
      "epoch 40  The loss is: 0.00612\n",
      "epoch 41  The loss is: 0.00593\n",
      "epoch 41  The loss is: 0.00738\n",
      "epoch 41  The loss is: 0.00648\n",
      "epoch 41  The loss is: 0.00730\n",
      "epoch 42  The loss is: 0.00539\n",
      "epoch 42  The loss is: 0.00526\n",
      "epoch 42  The loss is: 0.00656\n",
      "epoch 43  The loss is: 0.00688\n",
      "epoch 43  The loss is: 0.00458\n",
      "epoch 43  The loss is: 0.00545\n",
      "epoch 43  The loss is: 0.00530\n",
      "epoch 44  The loss is: 0.00557\n",
      "epoch 44  The loss is: 0.00544\n",
      "epoch 44  The loss is: 0.00483\n",
      "epoch 44  The loss is: 0.00488\n",
      "epoch 45  The loss is: 0.00464\n",
      "epoch 45  The loss is: 0.00498\n",
      "epoch 45  The loss is: 0.00582\n",
      "epoch 45  The loss is: 0.00417\n",
      "epoch 46  The loss is: 0.00474\n",
      "epoch 46  The loss is: 0.00464\n",
      "epoch 46  The loss is: 0.00542\n",
      "epoch 47  The loss is: 0.00443\n",
      "epoch 47  The loss is: 0.00441\n",
      "epoch 47  The loss is: 0.00363\n",
      "epoch 47  The loss is: 0.00535\n",
      "epoch 48  The loss is: 0.00410\n",
      "epoch 48  The loss is: 0.00577\n",
      "epoch 48  The loss is: 0.00491\n",
      "epoch 48  The loss is: 0.00539\n",
      "epoch 49  The loss is: 0.00533\n",
      "epoch 49  The loss is: 0.00410\n",
      "epoch 49  The loss is: 0.00333\n",
      "epoch 49  The loss is: 0.00552\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0001\n",
    "num_epoch = 50\n",
    "\n",
    "model = TextCNN(300, 24, 10, 0.5)\n",
    "weight = torch.FloatTensor(weibo.word_embedd)\n",
    "embeds = nn.Embedding.from_pretrained(weight)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "count = 0\n",
    "loss_sum = 0\n",
    "\n",
    "for epoch in tqdm(range(num_epoch)):\n",
    "    for data, label in use_data[3][0]:  # use_data[0][0] is the train part\n",
    "        \n",
    "        input_data = embeds(data)\n",
    "        out = model(input_data)\n",
    "        loss = criterion(out, label)\n",
    "        \n",
    "        loss_sum += loss\n",
    "        count += 1\n",
    "\n",
    "        if count % 100 == 0:\n",
    "            print(\"epoch\", epoch, end='  ')\n",
    "            print(\"The loss is: %.5f\" % (loss_sum/1000))\n",
    "\n",
    "            loss_sum = 0\n",
    "            count = 0\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fa90effe-a1d0-4ba5-97ce-651750b27577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 24])\n",
      "Accuracy: 84.08602142333984 %\n",
      "Precision: 83.07292175292969 %\n",
      "Recall: 97.256103515625 %\n",
      "F1 Score: 89.60675048828125 %\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    for texts, labels in use_data[3][1]:\n",
    "        if total == 0:\n",
    "            print(texts.shape)\n",
    "        outputs = model(embeds(texts))\n",
    "        _, predicted = torch.max(outputs.data, 1)  # the location of the max outputs\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.data).sum()\n",
    "        TP += ((predicted == 1) & (labels.data == 1)).sum()\n",
    "        TN += ((predicted == 0) & (labels.data == 0)).sum()\n",
    "        FN += ((predicted == 0) & (labels.data == 1)).sum()\n",
    "        FP += ((predicted == 1) & (labels.data == 0)).sum()\n",
    "        \n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    F1 = 2 * recall * precision / (recall + precision)\n",
    "    print('Accuracy: {} %'.format(100 * correct / total))\n",
    "    print('Precision: {} %'.format(100 * precision))\n",
    "    print('Recall: {} %'.format(100 * recall))\n",
    "    print('F1 Score: {} %'.format(100 * F1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b59f3c-7a60-4686-aca6-d7e70880bc41",
   "metadata": {},
   "source": [
    "## Multiclass Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "efb556ff-c2b3-40fe-98ff-8b22b20edfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "weibo_3 = Corpus()\n",
    "weibo_3.load_data(dataset_3[:,0], stopwords)\n",
    "weibo_3.load_pre_trained_embedding('data/sgns.weibo.char', 300)\n",
    "weibo_3.gen_dataset(dataset_3[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1621c445-4cab-40a6-8146-64b02177c0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14346\n",
      "10807\n"
     ]
    }
   ],
   "source": [
    "print(len(weibo_3.word_to_index))\n",
    "print(weibo_3.paired_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0e977669-5258-49ab-9546-4a7472b27faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3396\n"
     ]
    }
   ],
   "source": [
    "total_data_3 = weibo_3.result\n",
    "print(len(total_data_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1276da77-0617-4060-9f9d-18de585409db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN_3(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dimen, sentence_len, num_filters, dropout):\n",
    "        super(TextCNN_3, self).__init__()\n",
    "        self.conv3 = nn.Conv2d(1, num_filters, (3, embedding_dimen))\n",
    "        self.conv4 = nn.Conv2d(1, num_filters, (4, embedding_dimen))\n",
    "        self.conv5 = nn.Conv2d(1, num_filters, (5, embedding_dimen))\n",
    "        self.Max3_pool = nn.MaxPool2d((sentence_len-3+1, 1))\n",
    "        self.Max4_pool = nn.MaxPool2d((sentence_len-4+1, 1))\n",
    "        self.Max5_pool = nn.MaxPool2d((sentence_len-5+1, 1))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(3*num_filters, 3)   # 3 is the number of class\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch = x.shape[0]\n",
    "        # Convolution\n",
    "        x = x.unsqueeze(1)\n",
    "        x1 = F.relu(self.conv3(x))\n",
    "        x2 = F.relu(self.conv4(x))\n",
    "        x3 = F.relu(self.conv5(x))\n",
    "\n",
    "        # Pooling\n",
    "        x1 = self.Max3_pool(x1)\n",
    "        x2 = self.Max4_pool(x2)\n",
    "        x3 = self.Max5_pool(x3)\n",
    "\n",
    "        # capture and concatenate the features\n",
    "        x = torch.cat((x1, x2, x3), -1)\n",
    "        # print(x.shape)\n",
    "        x = x.view(batch, 1, -1)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # project the features to the labels\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, 3)  # 3 is the number of the label\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1ebe6ee5-8752-4484-b115-fa3bf6a72e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=False)\n",
    "\n",
    "use_data = {}\n",
    "\n",
    "num = 0\n",
    "for train_index, test_index in kf.split(dataset_3):\n",
    "    train = torch.utils.data.DataLoader(dataset=[total_data_3[i] for i in train_index],    # load the data\n",
    "                                               batch_size=5, \n",
    "                                               shuffle=True)\n",
    "    test = torch.utils.data.DataLoader(dataset=[total_data_3[i] for i in test_index],    # load the data\n",
    "                                               batch_size=5, \n",
    "                                               shuffle=True)\n",
    "    use_data[num] = [train, test]\n",
    "    num +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c89fd4a6-6cb8-417e-97e5-0e2649ea4665",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf9124f41195443c824203071a9e1136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  The loss is: 0.10319\n",
      "epoch 0  The loss is: 0.10352\n",
      "epoch 0  The loss is: 0.09857\n",
      "epoch 0  The loss is: 0.09802\n",
      "epoch 0  The loss is: 0.09805\n",
      "epoch 1  The loss is: 0.09147\n",
      "epoch 1  The loss is: 0.09765\n",
      "epoch 1  The loss is: 0.09166\n",
      "epoch 1  The loss is: 0.09193\n",
      "epoch 1  The loss is: 0.08736\n",
      "epoch 2  The loss is: 0.09291\n",
      "epoch 2  The loss is: 0.08840\n",
      "epoch 2  The loss is: 0.08491\n",
      "epoch 2  The loss is: 0.09080\n",
      "epoch 2  The loss is: 0.08640\n",
      "epoch 2  The loss is: 0.08417\n",
      "epoch 3  The loss is: 0.08484\n",
      "epoch 3  The loss is: 0.08260\n",
      "epoch 3  The loss is: 0.08479\n",
      "epoch 3  The loss is: 0.08082\n",
      "epoch 3  The loss is: 0.08183\n",
      "epoch 4  The loss is: 0.08245\n",
      "epoch 4  The loss is: 0.08237\n",
      "epoch 4  The loss is: 0.08427\n",
      "epoch 4  The loss is: 0.07805\n",
      "epoch 4  The loss is: 0.07967\n",
      "epoch 4  The loss is: 0.07512\n",
      "epoch 5  The loss is: 0.07582\n",
      "epoch 5  The loss is: 0.08041\n",
      "epoch 5  The loss is: 0.08039\n",
      "epoch 5  The loss is: 0.07411\n",
      "epoch 5  The loss is: 0.07917\n",
      "epoch 6  The loss is: 0.07535\n",
      "epoch 6  The loss is: 0.07638\n",
      "epoch 6  The loss is: 0.07409\n",
      "epoch 6  The loss is: 0.07801\n",
      "epoch 6  The loss is: 0.07640\n",
      "epoch 6  The loss is: 0.07577\n",
      "epoch 7  The loss is: 0.07040\n",
      "epoch 7  The loss is: 0.07235\n",
      "epoch 7  The loss is: 0.07703\n",
      "epoch 7  The loss is: 0.07560\n",
      "epoch 7  The loss is: 0.07107\n",
      "epoch 8  The loss is: 0.07397\n",
      "epoch 8  The loss is: 0.07283\n",
      "epoch 8  The loss is: 0.06982\n",
      "epoch 8  The loss is: 0.06823\n",
      "epoch 8  The loss is: 0.07822\n",
      "epoch 9  The loss is: 0.07285\n",
      "epoch 9  The loss is: 0.06833\n",
      "epoch 9  The loss is: 0.07067\n",
      "epoch 9  The loss is: 0.07166\n",
      "epoch 9  The loss is: 0.07127\n",
      "epoch 9  The loss is: 0.06846\n",
      "epoch 10  The loss is: 0.06933\n",
      "epoch 10  The loss is: 0.06789\n",
      "epoch 10  The loss is: 0.06921\n",
      "epoch 10  The loss is: 0.06689\n",
      "epoch 10  The loss is: 0.07145\n",
      "epoch 11  The loss is: 0.06512\n",
      "epoch 11  The loss is: 0.06430\n",
      "epoch 11  The loss is: 0.06901\n",
      "epoch 11  The loss is: 0.06507\n",
      "epoch 11  The loss is: 0.06860\n",
      "epoch 11  The loss is: 0.06801\n",
      "epoch 12  The loss is: 0.06777\n",
      "epoch 12  The loss is: 0.06346\n",
      "epoch 12  The loss is: 0.06764\n",
      "epoch 12  The loss is: 0.06324\n",
      "epoch 12  The loss is: 0.06620\n",
      "epoch 13  The loss is: 0.06053\n",
      "epoch 13  The loss is: 0.06558\n",
      "epoch 13  The loss is: 0.06543\n",
      "epoch 13  The loss is: 0.06282\n",
      "epoch 13  The loss is: 0.05696\n",
      "epoch 13  The loss is: 0.06486\n",
      "epoch 14  The loss is: 0.06565\n",
      "epoch 14  The loss is: 0.06089\n",
      "epoch 14  The loss is: 0.06274\n",
      "epoch 14  The loss is: 0.06033\n",
      "epoch 14  The loss is: 0.06349\n",
      "epoch 15  The loss is: 0.06344\n",
      "epoch 15  The loss is: 0.05954\n",
      "epoch 15  The loss is: 0.05881\n",
      "epoch 15  The loss is: 0.05923\n",
      "epoch 15  The loss is: 0.06437\n",
      "epoch 15  The loss is: 0.06316\n",
      "epoch 16  The loss is: 0.05823\n",
      "epoch 16  The loss is: 0.05886\n",
      "epoch 16  The loss is: 0.05582\n",
      "epoch 16  The loss is: 0.06349\n",
      "epoch 16  The loss is: 0.05813\n",
      "epoch 17  The loss is: 0.06164\n",
      "epoch 17  The loss is: 0.05743\n",
      "epoch 17  The loss is: 0.05588\n",
      "epoch 17  The loss is: 0.05689\n",
      "epoch 17  The loss is: 0.06082\n",
      "epoch 18  The loss is: 0.05850\n",
      "epoch 18  The loss is: 0.05996\n",
      "epoch 18  The loss is: 0.05882\n",
      "epoch 18  The loss is: 0.05399\n",
      "epoch 18  The loss is: 0.05213\n",
      "epoch 18  The loss is: 0.05970\n",
      "epoch 19  The loss is: 0.05699\n",
      "epoch 19  The loss is: 0.05656\n",
      "epoch 19  The loss is: 0.05355\n",
      "epoch 19  The loss is: 0.05643\n",
      "epoch 19  The loss is: 0.05743\n",
      "epoch 20  The loss is: 0.05652\n",
      "epoch 20  The loss is: 0.05736\n",
      "epoch 20  The loss is: 0.05418\n",
      "epoch 20  The loss is: 0.05203\n",
      "epoch 20  The loss is: 0.06053\n",
      "epoch 20  The loss is: 0.05570\n",
      "epoch 21  The loss is: 0.05355\n",
      "epoch 21  The loss is: 0.05307\n",
      "epoch 21  The loss is: 0.05030\n",
      "epoch 21  The loss is: 0.05578\n",
      "epoch 21  The loss is: 0.05320\n",
      "epoch 22  The loss is: 0.05620\n",
      "epoch 22  The loss is: 0.05136\n",
      "epoch 22  The loss is: 0.05544\n",
      "epoch 22  The loss is: 0.05662\n",
      "epoch 22  The loss is: 0.05257\n",
      "epoch 22  The loss is: 0.05167\n",
      "epoch 23  The loss is: 0.05070\n",
      "epoch 23  The loss is: 0.04828\n",
      "epoch 23  The loss is: 0.05146\n",
      "epoch 23  The loss is: 0.05654\n",
      "epoch 23  The loss is: 0.05113\n",
      "epoch 24  The loss is: 0.05431\n",
      "epoch 24  The loss is: 0.04967\n",
      "epoch 24  The loss is: 0.04877\n",
      "epoch 24  The loss is: 0.05284\n",
      "epoch 24  The loss is: 0.05190\n",
      "epoch 24  The loss is: 0.05117\n",
      "epoch 25  The loss is: 0.04974\n",
      "epoch 25  The loss is: 0.04838\n",
      "epoch 25  The loss is: 0.04889\n",
      "epoch 25  The loss is: 0.04703\n",
      "epoch 25  The loss is: 0.05379\n",
      "epoch 26  The loss is: 0.04830\n",
      "epoch 26  The loss is: 0.04891\n",
      "epoch 26  The loss is: 0.04745\n",
      "epoch 26  The loss is: 0.04998\n",
      "epoch 26  The loss is: 0.04933\n",
      "epoch 27  The loss is: 0.05251\n",
      "epoch 27  The loss is: 0.04555\n",
      "epoch 27  The loss is: 0.04459\n",
      "epoch 27  The loss is: 0.04569\n",
      "epoch 27  The loss is: 0.05220\n",
      "epoch 27  The loss is: 0.04791\n",
      "epoch 28  The loss is: 0.04945\n",
      "epoch 28  The loss is: 0.04460\n",
      "epoch 28  The loss is: 0.05178\n",
      "epoch 28  The loss is: 0.04533\n",
      "epoch 28  The loss is: 0.04495\n",
      "epoch 29  The loss is: 0.04412\n",
      "epoch 29  The loss is: 0.04825\n",
      "epoch 29  The loss is: 0.04376\n",
      "epoch 29  The loss is: 0.04750\n",
      "epoch 29  The loss is: 0.04734\n",
      "epoch 29  The loss is: 0.04564\n",
      "epoch 30  The loss is: 0.04458\n",
      "epoch 30  The loss is: 0.04090\n",
      "epoch 30  The loss is: 0.04520\n",
      "epoch 30  The loss is: 0.04001\n",
      "epoch 30  The loss is: 0.04645\n",
      "epoch 31  The loss is: 0.04766\n",
      "epoch 31  The loss is: 0.04568\n",
      "epoch 31  The loss is: 0.04559\n",
      "epoch 31  The loss is: 0.04274\n",
      "epoch 31  The loss is: 0.04177\n",
      "epoch 31  The loss is: 0.04314\n",
      "epoch 32  The loss is: 0.04117\n",
      "epoch 32  The loss is: 0.04454\n",
      "epoch 32  The loss is: 0.04111\n",
      "epoch 32  The loss is: 0.04175\n",
      "epoch 32  The loss is: 0.04463\n",
      "epoch 33  The loss is: 0.04226\n",
      "epoch 33  The loss is: 0.03840\n",
      "epoch 33  The loss is: 0.04293\n",
      "epoch 33  The loss is: 0.04346\n",
      "epoch 33  The loss is: 0.04171\n",
      "epoch 34  The loss is: 0.04008\n",
      "epoch 34  The loss is: 0.04325\n",
      "epoch 34  The loss is: 0.03784\n",
      "epoch 34  The loss is: 0.04285\n",
      "epoch 34  The loss is: 0.03956\n",
      "epoch 34  The loss is: 0.04238\n",
      "epoch 35  The loss is: 0.03904\n",
      "epoch 35  The loss is: 0.03731\n",
      "epoch 35  The loss is: 0.03982\n",
      "epoch 35  The loss is: 0.04001\n",
      "epoch 35  The loss is: 0.04576\n",
      "epoch 36  The loss is: 0.03669\n",
      "epoch 36  The loss is: 0.03664\n",
      "epoch 36  The loss is: 0.04175\n",
      "epoch 36  The loss is: 0.03904\n",
      "epoch 36  The loss is: 0.03691\n",
      "epoch 36  The loss is: 0.03789\n",
      "epoch 37  The loss is: 0.03575\n",
      "epoch 37  The loss is: 0.03983\n",
      "epoch 37  The loss is: 0.03726\n",
      "epoch 37  The loss is: 0.03986\n",
      "epoch 37  The loss is: 0.03782\n",
      "epoch 38  The loss is: 0.03791\n",
      "epoch 38  The loss is: 0.03826\n",
      "epoch 38  The loss is: 0.03804\n",
      "epoch 38  The loss is: 0.03684\n",
      "epoch 38  The loss is: 0.03977\n",
      "epoch 38  The loss is: 0.03895\n",
      "epoch 39  The loss is: 0.03469\n",
      "epoch 39  The loss is: 0.03805\n",
      "epoch 39  The loss is: 0.03875\n",
      "epoch 39  The loss is: 0.03886\n",
      "epoch 39  The loss is: 0.03743\n",
      "epoch 40  The loss is: 0.03368\n",
      "epoch 40  The loss is: 0.03581\n",
      "epoch 40  The loss is: 0.03921\n",
      "epoch 40  The loss is: 0.03374\n",
      "epoch 40  The loss is: 0.03575\n",
      "epoch 40  The loss is: 0.03592\n",
      "epoch 41  The loss is: 0.03254\n",
      "epoch 41  The loss is: 0.03388\n",
      "epoch 41  The loss is: 0.03531\n",
      "epoch 41  The loss is: 0.04061\n",
      "epoch 41  The loss is: 0.03373\n",
      "epoch 42  The loss is: 0.03661\n",
      "epoch 42  The loss is: 0.03192\n",
      "epoch 42  The loss is: 0.03601\n",
      "epoch 42  The loss is: 0.03434\n",
      "epoch 42  The loss is: 0.03325\n",
      "epoch 43  The loss is: 0.03375\n",
      "epoch 43  The loss is: 0.03475\n",
      "epoch 43  The loss is: 0.03706\n",
      "epoch 43  The loss is: 0.03383\n",
      "epoch 43  The loss is: 0.03136\n",
      "epoch 43  The loss is: 0.03564\n",
      "epoch 44  The loss is: 0.03566\n",
      "epoch 44  The loss is: 0.02997\n",
      "epoch 44  The loss is: 0.02873\n",
      "epoch 44  The loss is: 0.03346\n",
      "epoch 44  The loss is: 0.03234\n",
      "epoch 45  The loss is: 0.03247\n",
      "epoch 45  The loss is: 0.03603\n",
      "epoch 45  The loss is: 0.02903\n",
      "epoch 45  The loss is: 0.03404\n",
      "epoch 45  The loss is: 0.03301\n",
      "epoch 45  The loss is: 0.03033\n",
      "epoch 46  The loss is: 0.03118\n",
      "epoch 46  The loss is: 0.03261\n",
      "epoch 46  The loss is: 0.03365\n",
      "epoch 46  The loss is: 0.03219\n",
      "epoch 46  The loss is: 0.03274\n",
      "epoch 47  The loss is: 0.03091\n",
      "epoch 47  The loss is: 0.03104\n",
      "epoch 47  The loss is: 0.03091\n",
      "epoch 47  The loss is: 0.02992\n",
      "epoch 47  The loss is: 0.03307\n",
      "epoch 47  The loss is: 0.02997\n",
      "epoch 48  The loss is: 0.03247\n",
      "epoch 48  The loss is: 0.03110\n",
      "epoch 48  The loss is: 0.03002\n",
      "epoch 48  The loss is: 0.03006\n",
      "epoch 48  The loss is: 0.03092\n",
      "epoch 49  The loss is: 0.03054\n",
      "epoch 49  The loss is: 0.02795\n",
      "epoch 49  The loss is: 0.02911\n",
      "epoch 49  The loss is: 0.02857\n",
      "epoch 49  The loss is: 0.03277\n",
      "epoch 49  The loss is: 0.02982\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0001\n",
    "num_epoch = 50\n",
    "\n",
    "model = TextCNN_3(300, 24, 10, 0.5)\n",
    "weight = torch.FloatTensor(weibo_3.word_embedd)\n",
    "embeds = nn.Embedding.from_pretrained(weight)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "count = 0\n",
    "loss_sum = 0\n",
    "\n",
    "for epoch in tqdm(range(num_epoch)):\n",
    "    for data, label in use_data[4][0]:  # use_data[0][0] is the train part\n",
    "        \n",
    "        input_data = embeds(data)\n",
    "        out = model(input_data)\n",
    "        loss = criterion(out, label)\n",
    "        \n",
    "        loss_sum += loss\n",
    "        count += 1\n",
    "\n",
    "        if count % 100 == 0:\n",
    "            print(\"epoch\", epoch, end='  ')\n",
    "            print(\"The loss is: %.5f\" % (loss_sum/1000))\n",
    "\n",
    "            loss_sum = 0\n",
    "            count = 0\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "09d67d39-570f-4f9f-8bb7-40f463e97dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 24])\n",
      "Accuracy: 68.7776107788086 %\n",
      "Precision-macro: 66.75790405273438 %\n",
      "Recall-macro: 66.07547760009766 %\n",
      "F1 Score-macro: 65.99923706054688 %\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    TP_0 = 0\n",
    "    TN_0 = 0\n",
    "    FP_0 = 0\n",
    "    FN_0 = 0    \n",
    "    TP_1 = 0\n",
    "    TN_1 = 0\n",
    "    FP_1 = 0\n",
    "    FN_1 = 0\n",
    "    TP_2 = 0\n",
    "    TN_2 = 0\n",
    "    FP_2 = 0\n",
    "    FN_2 = 0\n",
    "    for texts, labels in use_data[4][1]:\n",
    "        if total == 0:\n",
    "            print(texts.shape)\n",
    "        outputs = model(embeds(texts))\n",
    "        _, predicted = torch.max(outputs.data, 1)  # the location of the max outputs\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.data).sum()\n",
    "        TP_0 += ((predicted == 0) & (labels.data == 0)).sum()\n",
    "        TN_0 += ((predicted != 0) & (labels.data != 0)).sum()\n",
    "        FN_0 += ((predicted != 0) & (labels.data == 0)).sum()\n",
    "        FP_0 += ((predicted == 0) & (labels.data != 0)).sum()\n",
    "        \n",
    "        TP_1 += ((predicted == 1) & (labels.data == 1)).sum()\n",
    "        TN_1 += ((predicted != 1) & (labels.data != 1)).sum()\n",
    "        FN_1 += ((predicted != 1) & (labels.data == 1)).sum()\n",
    "        FP_1 += ((predicted == 1) & (labels.data != 1)).sum()\n",
    "        \n",
    "        TP_2 += ((predicted == 2) & (labels.data == 2)).sum()\n",
    "        TN_2 += ((predicted != 2) & (labels.data != 2)).sum()\n",
    "        FN_2 += ((predicted != 2) & (labels.data == 2)).sum()\n",
    "        FP_2 += ((predicted == 2) & (labels.data != 2)).sum()\n",
    "        \n",
    "    precision_0 = TP_0 / (TP_0 + FP_0)\n",
    "    precision_1 = TP_1 / (TP_1 + FP_1)   \n",
    "    precision_2 = TP_2 / (TP_2 + FP_2)    \n",
    "    recall_0 = TP_0 / (TP_0 + FN_0)\n",
    "    recall_1 = TP_1 / (TP_1 + FN_1)  \n",
    "    recall_2 = TP_2 / (TP_2 + FN_2)\n",
    "    \n",
    "    F1_0 = 2 * recall_0 * precision_0 / (recall_0 + precision_0)\n",
    "    F1_1 = 2 * recall_1 * precision_1 / (recall_1 + precision_1)\n",
    "    F1_2 = 2 * recall_2 * precision_2 / (recall_2 + precision_2)\n",
    "    print('Accuracy: {} %'.format(100 * correct / total))\n",
    "    print('Precision-macro: {} %'.format(100 * (precision_0+precision_1+precision_2)/3))\n",
    "    print('Recall-macro: {} %'.format(100 * (recall_0+recall_1+recall_2)/3))\n",
    "    print('F1 Score-macro: {} %'.format(100 * (F1_0+F1_1+F1_2)/3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "80faba42-9878-4f69-a5d2-8118dbb8ce95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary classification:\n",
      "average accuracy of cross validation: 86.92009582519532 %\n",
      "average F1 score macro of cross validation: 91.44656524658203 %\n"
     ]
    }
   ],
   "source": [
    "print('Binary classification:')\n",
    "print('average accuracy of cross validation:', (85.59140014648438 + 87.74193572998047 + 87.95698547363281 + 89.22413635253906 + 84.08602142333984)/5,'%')\n",
    "print('average F1 score macro of cross validation:', (90.5233383178711+92.20245361328125+92.02279663085938+92.87748718261719+89.60675048828125)/5, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "a40878fe-6bfe-49f8-8f72-a95d92c92cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-Class classification:\n",
      "average accuracy of cross validation: 66.84345397949218 %\n",
      "average F1 score macro of cross validation: 62.631501007080075 %\n"
     ]
    }
   ],
   "source": [
    "print('3-Class classification:')\n",
    "print('average accuracy of cross validation:', (66.47058868408203 + 64.65390014648438+ 67.89395904541016 + 66.42121124267578 + 68.7776107788086)/5,'%')\n",
    "print('average F1 score macro of cross validation:', (61.498905181884766 + 59.06038284301758 + 63.84635925292969 + 62.752620697021484 + 65.99923706054688)/5, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562d4cd8-542e-4025-a142-d7463117f43f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
